{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries to import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn \n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn_deltatfidf import DeltaTfidfVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "import random\n",
    "from math import ceil, floor\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode, median, mean\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import json\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading input training file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3426, 3426, 3426)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_Data_Path = 'train_articles.json'\n",
    "with  open(T_Data_Path) as File:\n",
    "    Raw_T_Data = json.load(File)\n",
    "    \n",
    "Documents_D = Raw_T_Data['body']\n",
    "Titles_D = Raw_T_Data['title']\n",
    "Tags_D = Raw_T_Data['tags']\n",
    "Tags_Vector = []\n",
    "for Tag in Tags_D:\n",
    "    for T in Tags_D[Tag]:\n",
    "        if T not in Tags_Vector:\n",
    "            Tags_Vector.append(T)\n",
    "Documents = []\n",
    "Titles = []\n",
    "Tags = []\n",
    "for Index in Tags_D:\n",
    "    Documents.append(Documents_D[Index])\n",
    "    Titles.append(Titles_D[Index])\n",
    "    Tags.append(Tags_D[Index])\n",
    "len(Documents), len(Titles), len(Tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Tag_Convertor = {\n",
    "    'Mongodb':'Database'\n",
    "    , 'Database Development':'Database'\n",
    "    , 'Sql':'Database'\n",
    "    , 'MySQL':'Database'\n",
    "    , 'Postgresql':'Database'\n",
    "    , 'NoSQL':'Database'\n",
    "    , 'Parenting':'Family'\n",
    "    , 'Childhood':'Family'\n",
    "    , 'Children':'Family'\n",
    "    , 'Logo Design':'Graphic Design'\n",
    "    , 'Neural Networks':'Deep Learning'\n",
    "    , 'Startup Lessons': 'Entrepreneurship'\n",
    "    , 'Entrepreneur': 'Entrepreneurship'\n",
    "    , 'Jupyter': 'Python'\n",
    "    , 'Application Security':'Information Security'\n",
    "    , 'Ransomware':'Information Security'\n",
    "    , 'Machine Learning': 'ML'\n",
    "    , 'AndroidDev':'Android'\n",
    "    , 'Android Architecture' : 'Android'\n",
    "    , 'Android Studio' : 'Android'\n",
    "    , 'Android Apps' : 'Android'\n",
    "    , 'Web': 'Web Development'\n",
    "    , 'Artificial Intelligence':'AI'\n",
    "    , 'Open Source Software': 'Software'\n",
    "    , 'Software Architecture': 'Software'\n",
    "    , 'Software Development': 'Software'\n",
    "    , 'Software Engineering': 'Software'\n",
    "    , 'Software Testing': 'Software'\n",
    "    , 'Career Change':'Career'\n",
    "    , 'Career Advice':'Career'\n",
    "    , 'Careers':'Career'\n",
    "    , 'Ux Trends': 'UX'\n",
    "    , 'Storytelling' : 'Writing'\n",
    "    , 'Film': 'Movies'\n",
    "    , 'Startups': 'Startup'\n",
    "    , 'UI Design' : 'UI'\n",
    "    , 'Humor' : 'Comedy'\n",
    "    , 'Satire' : 'Comedy'\n",
    "    , 'Humour' : 'Comedy'\n",
    "    , 'Jokes' : 'Comedy'\n",
    "    , 'Life Lessons' : 'Life'\n",
    "    , 'Christmas Costume' : 'Christmas'\n",
    "    , 'Design Digest' : 'Design'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Android Apps': 'Android',\n",
       " 'Android Architecture': 'Android',\n",
       " 'Android Studio': 'Android',\n",
       " 'AndroidDev': 'Android',\n",
       " 'Application Security': 'Information Security',\n",
       " 'Artificial Intelligence': 'AI',\n",
       " 'Career Advice': 'Career',\n",
       " 'Career Change': 'Career',\n",
       " 'Careers': 'Career',\n",
       " 'Childhood': 'Family',\n",
       " 'Children': 'Family',\n",
       " 'Christmas Costume': 'Christmas',\n",
       " 'Database Development': 'Database',\n",
       " 'Design Digest': 'Design',\n",
       " 'Entrepreneur': 'Entrepreneurship',\n",
       " 'Film': 'Movies',\n",
       " 'Humor': 'Comedy',\n",
       " 'Humour': 'Comedy',\n",
       " 'Jokes': 'Comedy',\n",
       " 'Jupyter': 'Python',\n",
       " 'Life Lessons': 'Life',\n",
       " 'Logo Design': 'Graphic Design',\n",
       " 'Machine Learning': 'ML',\n",
       " 'Mongodb': 'Database',\n",
       " 'MySQL': 'Database',\n",
       " 'Neural Networks': 'Deep Learning',\n",
       " 'NoSQL': 'Database',\n",
       " 'Open Source Software': 'Software',\n",
       " 'Parenting': 'Family',\n",
       " 'Postgresql': 'Database',\n",
       " 'Ransomware': 'Information Security',\n",
       " 'Satire': 'Comedy',\n",
       " 'Software Architecture': 'Software',\n",
       " 'Software Development': 'Software',\n",
       " 'Software Engineering': 'Software',\n",
       " 'Software Testing': 'Software',\n",
       " 'Sql': 'Database',\n",
       " 'Startup Lessons': 'Entrepreneurship',\n",
       " 'Startups': 'Startup',\n",
       " 'Storytelling': 'Writing',\n",
       " 'UI Design': 'UI',\n",
       " 'Ux Trends': 'UX',\n",
       " 'Web': 'Web Development'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tag_Convertor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter documents that have no tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3218, 3218, 3218)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NoTags = []\n",
    "for i, TL in enumerate(Tags):\n",
    "    if len(TL) == 0:\n",
    "        NoTags.append(i)\n",
    "Documents = np.delete(Documents, NoTags)\n",
    "Titles = np.delete(Titles, NoTags)\n",
    "Tags = np.delete(Tags, NoTags)\n",
    "\n",
    "len(Documents), len(Titles), len(Tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Know more about classes frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2998"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TGS = {}\n",
    "for TL in Tags:\n",
    "    for Tag in TL:\n",
    "        if Tag in TGS:\n",
    "            TGS[Tag] += 1\n",
    "        else:\n",
    "            TGS[Tag] = 1\n",
    "import operator            \n",
    "TGS = list(reversed(sorted(TGS.items(), key=operator.itemgetter(1))))\n",
    "len(TGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use tag convertor to reduce the effect of rare tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2955"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, TL in enumerate(Tags):\n",
    "    for k, T in enumerate(TL):\n",
    "        if T in Tag_Convertor:\n",
    "            if Tag_Convertor[T] not in TL:\n",
    "#                 print('Changed '+ T +' to ' + Tag_Convertor[T])\n",
    "                Tags[i][k] = Tag_Convertor[T]\n",
    "            else:\n",
    "                del Tags[i][k]\n",
    "                \n",
    "                \n",
    "# Revisulaize\n",
    "TGS = {}\n",
    "for TL in Tags:\n",
    "    for Tag in TL:\n",
    "        if Tag in TGS:\n",
    "            TGS[Tag] += 1\n",
    "        else:\n",
    "            TGS[Tag] = 1\n",
    "import operator            \n",
    "TGS = list(reversed(sorted(TGS.items(), key=operator.itemgetter(1))))\n",
    "len(TGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label binarizer\n",
    "## To convert tags to binary encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Label_Binarizer = preprocessing.MultiLabelBinarizer()\n",
    "Label_Binarizer.fit(Tags)\n",
    "\n",
    "Y = Label_Binarizer.transform(Tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Test_Train = 0\n",
    "if Test_Train:\n",
    "    X_Train, X_Test, Y_Train, Y_Test = train_test_split(Documents, Y, test_size= 0.1)\n",
    "else:\n",
    "    X_Train = Documents\n",
    "    Y_Train = Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF_IDF vectorizer with english stop words and nltk tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFiDFV = TfidfVectorizer(tokenizer=nltk.word_tokenize, stop_words=nltk.corpus.stopwords.words('english'))\n",
    "TFiDFV = TFiDFV.fit(X_Train)\n",
    "V_Train = TFiDFV.transform(X_Train)\n",
    "# V_Test = TFiDFV.transform(X_Test)\n",
    "V_Train.shape #, V_Test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Vs-Rest classifier with SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=1e-05,\n",
       "     verbose=0),\n",
       "          n_jobs=2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Warnings mean that certain classes has not been predicted true once in the whole training set (classes with so small frequency)\n",
    "M_LSVC = OneVsRestClassifier(LinearSVC(tol=0.00001, class_weight='balanced'), n_jobs=2)\n",
    "M_LSVC.fit(V_Train, Y_Train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Design', 'UX', 'Writing')]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Predictions = M_LSVC.predict(V_Test[276])\n",
    "Label_Binarizer.inverse_transform(Predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Label_Binarizer.inverse_transform(np.array([Y_Test[276]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = M_LSVC.predict(V_Test)\n",
    "Failed = 0\n",
    "Saved = open('TestFileTok_Stop_Con.txt', 'w')\n",
    "for p in predictions:\n",
    "    st = str(Label_Binarizer.inverse_transform(np.array([p])))\n",
    "    Saved.writelines([st, '\\n'])\n",
    "    if st == '[()]':\n",
    "        Failed += 1\n",
    "Saved.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Failed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Ti_TFiDFV = TfidfVectorizer(tokenizer=nltk.word_tokenize, stop_words=nltk.corpus.stopwords.words('english'))\n",
    "Ti_TFiDFV = Ti_TFiDFV.fit(Titles)\n",
    "Ti_V_Train = Ti_TFiDFV.transform(Titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=1e-05,\n",
       "     verbose=0),\n",
       "          n_jobs=2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ti_M_LSVC = OneVsRestClassifier(LinearSVC(tol=0.00001, class_weight='balanced'), n_jobs=2)\n",
    "Ti_M_LSVC.fit(Ti_V_Train, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Test_Data_Path = 'test_articles.json'\n",
    "with  open(Test_Data_Path) as File:\n",
    "    Raw_Test_Data = json.load(File)\n",
    "    \n",
    "Test_Documents_D = Raw_Test_Data['body']\n",
    "Test_Titles_D = Raw_Test_Data['title']\n",
    "\n",
    "Test_Documents = list(Test_Documents_D.values())\n",
    "Test_Titles = list(Test_Titles_D.values())\n",
    "\n",
    "len(Test_Documents), len(Test_Titles)\n",
    "\n",
    "V_Ti = Ti_TFiDFV.transform(Test_Titles)\n",
    "Predictions = M_LSVC.predict(V_Ti)\n",
    "Saved = open('Submission_Titles.txt', 'w')\n",
    "Failed = 0\n",
    "for p in Predictions:\n",
    "    st = str(Label_Binarizer.inverse_transform(np.array([p])))\n",
    "    Saved.writelines([st, '\\n'])\n",
    "    if st == '[()]':\n",
    "        Failed += 1\n",
    "Saved.close()\n",
    "Failed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_Score = M_LSVC.decision_function(V_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 2955\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "for i in range(n_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(Y_Test[:, i],\n",
    "                                                        Y_Score[:, i])\n",
    "    average_precision[i] = average_precision_score(Y_Test[:, i], Y_Score[:, i])\n",
    "\n",
    "# A \"micro-average\": quantifying score on all classes jointly\n",
    "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(Y_Test.ravel(),\n",
    "    Y_Score.ravel())\n",
    "average_precision[\"micro\"] = average_precision_score(Y_Test, Y_Score,\n",
    "                                                     average=\"micro\")\n",
    "print('Average precision score, micro-averaged over all classes: {0:0.2f}'\n",
    "      .format(average_precision[\"micro\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.step(recall['micro'], precision['micro'], color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall[\"micro\"], precision[\"micro\"], step='post', alpha=0.2,\n",
    "                 color='b')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title(\n",
    "    'Average precision score, micro-averaged over all classes: AP={0:0.2f}'\n",
    "    .format(average_precision[\"micro\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('M_LSVC_Titles_Balanced_.pkl', 'wb') as f:\n",
    "    pickle.dump(M_LSVC, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read test data for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Data_Path = 'test_articles.json'\n",
    "with  open(Test_Data_Path) as File:\n",
    "    Raw_Test_Data = json.load(File)\n",
    "    \n",
    "Test_Documents_D = Raw_Test_Data['body']\n",
    "Test_Titles_D = Raw_Test_Data['title']\n",
    "\n",
    "Test_Documents = list(Test_Documents_D.values())\n",
    "Test_Titles = list(Test_Titles_D.values())\n",
    "\n",
    "len(Test_Documents), len(Test_Titles)\n",
    "\n",
    "V_T = TFiDFV.transform(Test_Documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Predictions = M_LSVC.predict(V_T)\n",
    "Saved = open('Submission.txt', 'w')\n",
    "Failed = 0\n",
    "for p in Predictions:\n",
    "    st = str(Label_Binarizer.inverse_transform(np.array([p])))\n",
    "    Saved.writelines([st, '\\n'])\n",
    "    if st == '[()]':\n",
    "        Failed += 1\n",
    "Saved.close()\n",
    "Failed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Read the model and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Predictor = pickle.load(open('M_LSVC_Tok_Stop.pkl', 'rb'))\n",
    "predictions = Predictor.predict(V_Test)\n",
    "Saved = open('TestFileTok_Stop.txt', 'w')\n",
    "for p in predictions:\n",
    "    Saved.writelines([str(Label_Binarizer.inverse_transform(np.array([p]))), '\\n'])\n",
    "Saved.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final JSON Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_P = 'test_articles.json'\n",
    "with  open(Data_P) as File:\n",
    "    Data = json.load(File)\n",
    "    \n",
    "Docs = Data['body']\n",
    "Titls = Data['title']\n",
    "\n",
    "JSON_D = {}\n",
    "\n",
    "index = '7'\n",
    "\n",
    "Doc = Docs[index]\n",
    "Titl = Titls[index]\n",
    "\n",
    "Doc_TF = TFiDFV.transform([Doc])\n",
    "Doc_O = M_LSVC.predict(Doc_TF)\n",
    "Titl_O = Ti_M_LSVC.predict(Ti_TFiDFV.transform([Titl]))\n",
    "\n",
    "Output = Doc_O * Titl_O\n",
    "\n",
    "\n",
    "St = str(Label_Binarizer.inverse_transform(np.array(Output))[0])\n",
    "print(St)\n",
    "\n",
    "JSON_D[index] = St\n",
    "\n",
    "print(index)\n",
    "\n",
    "json.dump(JSON_D, open('Submission.json'))\n",
    "\n",
    "# for index in Docs:\n",
    "#     Doc = Docs[index]\n",
    "#     Titl = Titls[index]\n",
    "\n",
    "#     Doc_TF = TFiDFV.transform([Doc])\n",
    "#     Doc_O = M_LSVC.predict(Doc_TF)\n",
    "#     Titl_O = Ti_M_LSVC.predict(Ti_TFiDFV.transform([Titl]))\n",
    "    \n",
    "#     Output = Doc_O * Titl_O\n",
    "    \n",
    "    \n",
    "#     St = str(Label_Binarizer.inverse_transform(np.array(Output))[0])\n",
    "    \n",
    "#     JSON_D[index] = St\n",
    "    \n",
    "#     print(index)\n",
    "\n",
    "# json.dump(JSON_D, open('Submission.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = gensim.models.ldamodel.LdaModel(corpus=gensim.corpora.textcorpus(Documents), num_topics=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "len(stopwords.words('english'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
